{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "m6lyNHBYlPTA"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Replace with YOUR GROQ API key\n",
        "API_KEY = \"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\"\n",
        "API_URL = \"https://api.groq.com/openai/v1/chat/completions\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hx0KU1dsLXy"
      },
      "source": [
        "Use CURL to do api call from terminal:\n",
        "\n",
        "```\n",
        "curl --location \"https://api.groq.com/openai/v1/chat/completions\" --header \"Authorization: Bearer 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\" --header \"Content-Type: application/json\" --data \"{ \\\"model\\\": \\\"mixtral-8x7b-32768\\\", \\\"messages\\\": [ { \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the capital of France?\\\" } ] }\"\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "JMSuABjik149"
      },
      "outputs": [],
      "source": [
        "def get_groq_response(messages, model=\"mixtral-8x7b-32768\"): #or llama2-70b-4096 or any other models\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "    }\n",
        "\n",
        "    response = requests.post(API_URL, headers=headers, json=data)\n",
        "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnsjLg2olTmA",
        "outputId": "9dcdef78-fbb3-4e8d-c646-b1340f9dbf23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the GROQ chatbot! Type 'quit' to exit.\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "def get_groq_response(user_input, model=\"deepseek-r1-distill-llama-70b\"): #deepseek-r1-distill-llama-70b llama-3.1-8b-instant llama-3.3-70b-specdec\n",
        "    \"\"\"\n",
        "    Sends a single user input to the GROQ API and returns the response.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": user_input}],\n",
        "    }\n",
        "\n",
        "    response = requests.post(API_URL, headers=headers, json=data)\n",
        "    response.raise_for_status()\n",
        "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def simple_chatbot():\n",
        "    \"\"\"\n",
        "    A simple chatbot that interacts with the user (no memory).\n",
        "    \"\"\"\n",
        "    print(\"Welcome to the GROQ chatbot! Type 'quit' to exit.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        response = get_groq_response(user_input)\n",
        "\n",
        "        if response:\n",
        "            print(\"Chatbot:\", response)\n",
        "        else:\n",
        "            print(\"Sorry, I couldn't generate a response.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    simple_chatbot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh7EVUaamePA",
        "outputId": "060c2847-bf93-4336-9fbf-d02590f1c9b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the GROQ chatbot with memory! Type 'quit' to exit.\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "def get_groq_response(messages, model=\"deepseek-r1-distill-llama-70b\"): #deepseek-r1-distill-llama-70b llama-3.1-8b-instant llama-3.3-70b-specdec\n",
        "    \"\"\"\n",
        "    Sends a request to the GROQ API and returns the response.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "    }\n",
        "    response = requests.post(API_URL, headers=headers, json=data)\n",
        "    response.raise_for_status()\n",
        "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "def chatbot_with_memory():\n",
        "    \"\"\"\n",
        "    A chatbot that interacts with the user, remembering previous turns.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    print(\"Welcome to the GROQ chatbot with memory! Type 'quit' to exit.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "        response = get_groq_response(messages)\n",
        "\n",
        "        if response:\n",
        "            print(\"Chatbot:\", response)\n",
        "            messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "        else:\n",
        "            print(\"Sorry, I couldn't generate a response.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chatbot_with_memory()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
